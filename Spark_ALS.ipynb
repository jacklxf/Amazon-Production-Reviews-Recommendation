{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>productId</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AKM1MP6P0OYPR</td>\n",
       "      <td>0132793040</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A2CX7LUOHB2NDG</td>\n",
       "      <td>0321732944</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A2NWSAGRHCP8N5</td>\n",
       "      <td>0439886341</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2WNBOD3WNDNKT</td>\n",
       "      <td>0439886341</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1GI0U4ZRJA8WN</td>\n",
       "      <td>0439886341</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           userId   productId  Rating\n",
       "0   AKM1MP6P0OYPR  0132793040     5.0\n",
       "1  A2CX7LUOHB2NDG  0321732944     5.0\n",
       "2  A2NWSAGRHCP8N5  0439886341     1.0\n",
       "3  A2WNBOD3WNDNKT  0439886341     3.0\n",
       "4  A1GI0U4ZRJA8WN  0439886341     1.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('./data/ratings_Electronics.csv',names=['userId','productId','Rating','timestamp'])\n",
    "data=data[['userId','productId','Rating']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7824482 782448\n",
      "0.0999999744392025\n"
     ]
    }
   ],
   "source": [
    "newData=data.sample(frac=0.1,random_state=1)\n",
    "print(len(data),len(newData))\n",
    "print(len(newData)/len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('alsRec').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+------+\n",
      "|        userId| productId|Rating|\n",
      "+--------------+----------+------+\n",
      "| A9UYPR4Q055LZ|B00AFXUUV6|   1.0|\n",
      "| ACUWMWOHAFDXF|B00BFDHVAS|   5.0|\n",
      "| ADH0W9QWMJ8V7|B000OMKR8E|   4.0|\n",
      "|A1VDA4Z5EMT052|B0075SUG3Q|   5.0|\n",
      "| ASYZVGMBYVY2Z|B0006B486K|   2.0|\n",
      "| AJHVJMH379SQF|B0047UNP3I|   1.0|\n",
      "|A1CDZ07YBEZP6C|B0046HNWO4|   5.0|\n",
      "| APRVK6PDTNH82|B002U8573K|   3.0|\n",
      "|A3KD4DEAEO2KRF|B001UZJBGI|   5.0|\n",
      "|A2CGKM93KATTY3|B00746LVOM|   1.0|\n",
      "+--------------+----------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newData=spark.createDataFrame(newData)\n",
    "newData.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>userId</th>\n",
       "      <th>productId</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>782448</td>\n",
       "      <td>782448</td>\n",
       "      <td>782448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>None</td>\n",
       "      <td>5.370610959261765E9</td>\n",
       "      <td>4.012918430362146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>None</td>\n",
       "      <td>4.0989770434053845E9</td>\n",
       "      <td>1.3802497297953855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>A00018041RRVMCICCAP79</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>AZZZWXXUPZ1F3</td>\n",
       "      <td>BT008G3W52</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary                 userId             productId              Rating\n",
       "0   count                 782448                782448              782448\n",
       "1    mean                   None   5.370610959261765E9   4.012918430362146\n",
       "2  stddev                   None  4.0989770434053845E9  1.3802497297953855\n",
       "3     min  A00018041RRVMCICCAP79            0528881469                 1.0\n",
       "4     max          AZZZWXXUPZ1F3            BT008G3W52                 5.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newData.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- productId: string (nullable = true)\n",
      " |-- Rating: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782448"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|count(DISTINCT userId)|\n",
      "+----------------------+\n",
      "|                672618|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newData.select(countDistinct('userId')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol = 'userId', outputCol = \"moduserID\")\n",
    "DFF=indexer.fit(newData).transform(newData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol = 'productId', outputCol = \"modproductID\")\n",
    "DFF=indexer.fit(DFF).transform(DFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+------+---------+------------+\n",
      "|        userId| productId|Rating|moduserID|modproductID|\n",
      "+--------------+----------+------+---------+------------+\n",
      "| A9UYPR4Q055LZ|B00AFXUUV6|   1.0| 558262.0|     14326.0|\n",
      "| ACUWMWOHAFDXF|B00BFDHVAS|   5.0| 571418.0|      1021.0|\n",
      "| ADH0W9QWMJ8V7|B000OMKR8E|   4.0|  64513.0|       426.0|\n",
      "|A1VDA4Z5EMT052|B0075SUG3Q|   5.0| 211300.0|      8889.0|\n",
      "| ASYZVGMBYVY2Z|B0006B486K|   2.0| 642022.0|       328.0|\n",
      "| AJHVJMH379SQF|B0047UNP3I|   1.0| 600495.0|    119827.0|\n",
      "|A1CDZ07YBEZP6C|B0046HNWO4|   5.0| 128396.0|     65161.0|\n",
      "| APRVK6PDTNH82|B002U8573K|   3.0| 627917.0|     11214.0|\n",
      "|A3KD4DEAEO2KRF|B001UZJBGI|   5.0| 479855.0|    104154.0|\n",
      "|A2CGKM93KATTY3|B00746LVOM|   1.0| 286936.0|       410.0|\n",
      "+--------------+----------+------+---------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DFF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- productId: string (nullable = true)\n",
      " |-- Rating: double (nullable = true)\n",
      " |-- moduserID: double (nullable = false)\n",
      " |-- modproductID: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DFF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DFF.filter(DFF.modproductID.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "DFF=DFF.drop(*['userId','productId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------------+\n",
      "|Rating|moduserID|modproductID|\n",
      "+------+---------+------------+\n",
      "|   1.0| 558262.0|     14326.0|\n",
      "|   5.0| 571418.0|      1021.0|\n",
      "|   4.0|  64513.0|       426.0|\n",
      "|   5.0| 211300.0|      8889.0|\n",
      "|   2.0| 642022.0|       328.0|\n",
      "|   1.0| 600495.0|    119827.0|\n",
      "|   5.0| 128396.0|     65161.0|\n",
      "|   3.0| 627917.0|     11214.0|\n",
      "|   5.0| 479855.0|    104154.0|\n",
      "|   1.0| 286936.0|       410.0|\n",
      "+------+---------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DFF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, test = DFF.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o807.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 486.0 failed 1 times, most recent failure: Lost task 0.0 in stage 486.0 (TID 4926, 192.168.110.129, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.lang.reflect.Array.newInstance(Array.java:75)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:1996)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2344)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2268)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2126)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2344)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2268)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2126)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2032)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2344)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2268)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2126)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2032)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2344)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2268)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2126)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2344)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2268)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2126)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2344)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2268)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2126)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1227)\n\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:960)\n\tat org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:709)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:691)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.lang.reflect.Array.newInstance(Array.java:75)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:1996)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2344)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2268)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2126)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2344)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2268)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2126)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2032)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2344)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2268)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2126)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2032)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2344)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2268)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2126)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2344)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2268)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2126)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2344)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2268)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2126)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-ddc1843accb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m als = ALS(maxIter=20, regParam=0.01, userCol=\"moduserID\", itemCol=\"modproductID\", \n\u001b[1;32m      2\u001b[0m           ratingCol=\"Rating\",coldStartStrategy='drop')\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o807.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 486.0 failed 1 times, most recent failure: Lost task 0.0 in stage 486.0 (TID 4926, 192.168.110.129, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.lang.reflect.Array.newInstance(Array.java:75)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:1996)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2344)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2268)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2126)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2344)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2268)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2126)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2032)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2344)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2268)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2126)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2032)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2344)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2268)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2126)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2344)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2268)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2126)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2344)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2268)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2126)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1227)\n\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:960)\n\tat org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:709)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:691)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.lang.reflect.Array.newInstance(Array.java:75)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:1996)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2344)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2268)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2126)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2344)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2268)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2126)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2032)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2344)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2268)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2126)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2032)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2344)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2268)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2126)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2344)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2268)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2126)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2344)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2268)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2126)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 52876)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 720, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 268, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 245, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/lib/spark/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "als = ALS(maxIter=20, regParam=0.01, userCol=\"moduserID\", itemCol=\"modproductID\", \n",
    "          ratingCol=\"Rating\",coldStartStrategy='drop')\n",
    "model = als.fit(training)\n",
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>Rating</th>\n",
       "      <th>moduserID</th>\n",
       "      <th>modproductID</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>3019</td>\n",
       "      <td>3019</td>\n",
       "      <td>3019</td>\n",
       "      <td>3019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>4.192116594898973</td>\n",
       "      <td>4637.724412056972</td>\n",
       "      <td>7793.387214309374</td>\n",
       "      <td>-0.009166724631826894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>1.2036740165664686</td>\n",
       "      <td>3357.326941445099</td>\n",
       "      <td>8494.303117700896</td>\n",
       "      <td>1.6709708858234567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.698547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10888.0</td>\n",
       "      <td>31626.0</td>\n",
       "      <td>11.719991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary              Rating          moduserID       modproductID  \\\n",
       "0   count                3019               3019               3019   \n",
       "1    mean   4.192116594898973  4637.724412056972  7793.387214309374   \n",
       "2  stddev  1.2036740165664686  3357.326941445099  8494.303117700896   \n",
       "3     min                 1.0                0.0                0.0   \n",
       "4     max                 5.0            10888.0            31626.0   \n",
       "\n",
       "              prediction  \n",
       "0                   3019  \n",
       "1  -0.009166724631826894  \n",
       "2     1.6709708858234567  \n",
       "3             -12.698547  \n",
       "4              11.719991  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =predictions.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.6839928356071985"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol='Rating')\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "rmse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
